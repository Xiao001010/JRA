[Mon Aug 07 17:38:53 2023|main.py|INFO] Task: VAE-latentdim_128-sigma_0.001-bg_0.5
[Mon Aug 07 17:38:53 2023|main.py|INFO] Device: cuda
[Mon Aug 07 17:38:53 2023|main.py|INFO] Config path: config/VAE-latentdim_128-sigma_0.001-bg_0.5.yaml
[Mon Aug 07 17:38:53 2023|main.py|INFO] Log path: logs\VAE-latentdim_128-sigma_0.001-bg_0.5\2023-08-07-17-38-53\train.log
[Mon Aug 07 17:38:53 2023|main.py|INFO] Checkpoint path: checkpoints\VAE-latentdim_128-sigma_0.001-bg_0.5\2023-08-07-17-38-53
[Mon Aug 07 17:38:53 2023|main.py|INFO] Random seed: 42
[Mon Aug 07 17:38:53 2023|main.py|INFO] Batch size: 128
[Mon Aug 07 17:38:53 2023|main.py|INFO] Data path: data/single_cell_data_with_mask
[Mon Aug 07 17:38:53 2023|main.py|INFO] Number of workers: 4
[Mon Aug 07 17:38:53 2023|main.py|INFO] Shuffle: True
[Mon Aug 07 17:38:53 2023|main.py|INFO] Mean: [6076.686, 1350.9691, 5090.1455, 5019.978]
[Mon Aug 07 17:38:53 2023|main.py|INFO] Std: [5504.3955, 1145.6356, 663.3312, 706.004]
[Mon Aug 07 17:38:53 2023|main.py|INFO] Image size: 32
[Mon Aug 07 17:38:53 2023|main.py|INFO] Learning rate: 0.001
[Mon Aug 07 17:38:53 2023|main.py|INFO] Learning rate decay: 0.95
[Mon Aug 07 17:38:53 2023|main.py|INFO] In channels: 4
[Mon Aug 07 17:38:53 2023|main.py|INFO] Image size: 32
[Mon Aug 07 17:38:53 2023|main.py|INFO] Latent dim: 128
[Mon Aug 07 17:38:53 2023|main.py|INFO] Use batch norm: True
[Mon Aug 07 17:38:53 2023|main.py|INFO] Dropout rate: 0.0
[Mon Aug 07 17:38:53 2023|main.py|INFO] Layer list: [2, 2, 2, 2]
[Mon Aug 07 17:38:53 2023|main.py|INFO] Sigma: 0.001
[Mon Aug 07 17:38:53 2023|main.py|INFO] Background variance: 0.5
[Mon Aug 07 17:38:53 2023|main.py|INFO] Epochs: 500
[Mon Aug 07 17:38:53 2023|main.py|INFO] Checkpoint save interval (epochs): 10
[Mon Aug 07 17:38:53 2023|main.py|INFO] Train transform: Compose(
    ToTensor()
    Normalize(mean=[6076.686, 1350.9691, 5090.1455, 5019.978], std=[5504.3955, 1145.6356, 663.3312, 706.004])
    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)
)
[Mon Aug 07 17:38:53 2023|main.py|INFO] Train mask transform: Compose(
    ToTensor()
    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)
)
[Mon Aug 07 17:38:53 2023|main.py|INFO] Loading dataset from data/single_cell_data_with_mask ...
[Mon Aug 07 17:38:53 2023|main.py|INFO] Building model ...
[Mon Aug 07 17:38:53 2023|main.py|INFO] Model: ResVAE(
  (encoder): Encoder(
    (conv_1): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (norm_layer_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): LeakyReLU(negative_slope=0.01, inplace=True)
    (layer_1): Sequential(
      (0): EncoderBottleneckBlock(
        (conv_1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): Conv2d(8, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)
        (norm_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01, inplace=True)
        (downsample): Sequential(
          (0): Conv2d(8, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): EncoderBottleneckBlock(
        (conv_1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (norm_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (layer_2): Sequential(
      (0): EncoderBottleneckBlock(
        (conv_1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): Conv2d(8, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)
        (norm_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01, inplace=True)
        (downsample): Sequential(
          (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): EncoderBottleneckBlock(
        (conv_1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (norm_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (layer_3): Sequential(
      (0): EncoderBottleneckBlock(
        (conv_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): Conv2d(16, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)
        (norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01, inplace=True)
        (downsample): Sequential(
          (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): EncoderBottleneckBlock(
        (conv_1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (layer_4): Sequential(
      (0): EncoderBottleneckBlock(
        (conv_1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01, inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): EncoderBottleneckBlock(
        (conv_1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (conv_1x1): Sequential(
      (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (fc_mu): Linear(in_features=64, out_features=128, bias=True)
    (fc_var): Linear(in_features=64, out_features=128, bias=True)
  )
  (decoder): Decoder(
    (dense_1): Linear(in_features=128, out_features=64, bias=True)
    (layer_1): Sequential(
      (0): DecoderBottleneckBlock(
        (upsample): Sequential(
          (0): ConvTranspose2d(16, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv_1): ConvTranspose2d(16, 32, kernel_size=(1, 1), stride=(1, 1))
        (norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): ConvTranspose2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
        (norm_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (1): DecoderBottleneckBlock(
        (conv_1): ConvTranspose2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
        (norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): ConvTranspose2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
        (norm_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01)
      )
    )
    (layer_2): Sequential(
      (0): DecoderBottleneckBlock(
        (upsample): Sequential(
          (0): ConvTranspose2d(128, 128, kernel_size=(1, 1), stride=(2, 2), output_padding=(1, 1))
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv_1): ConvTranspose2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
        (norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
        (norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): ConvTranspose2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
        (norm_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (1): DecoderBottleneckBlock(
        (conv_1): ConvTranspose2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
        (norm_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): ConvTranspose2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
        (norm_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01)
      )
    )
    (layer_3): Sequential(
      (0): DecoderBottleneckBlock(
        (upsample): Sequential(
          (0): ConvTranspose2d(128, 64, kernel_size=(1, 1), stride=(2, 2), output_padding=(1, 1))
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv_1): ConvTranspose2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
        (norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): ConvTranspose2d(16, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
        (norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): ConvTranspose2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
        (norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (1): DecoderBottleneckBlock(
        (conv_1): ConvTranspose2d(64, 16, kernel_size=(1, 1), stride=(1, 1))
        (norm_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): ConvTranspose2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (norm_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): ConvTranspose2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
        (norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01)
      )
    )
    (layer_4): Sequential(
      (0): DecoderBottleneckBlock(
        (upsample): Sequential(
          (0): ConvTranspose2d(64, 32, kernel_size=(1, 1), stride=(2, 2), output_padding=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv_1): ConvTranspose2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
        (norm_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): ConvTranspose2d(8, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
        (norm_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): ConvTranspose2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
        (norm_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01)
      )
      (1): DecoderBottleneckBlock(
        (conv_1): ConvTranspose2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
        (norm_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): ConvTranspose2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (norm_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): ConvTranspose2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
        (norm_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01)
      )
    )
    (layer_5): Sequential(
      (0): DecoderBottleneckBlock(
        (upsample): Sequential(
          (0): ConvTranspose2d(32, 32, kernel_size=(1, 1), stride=(2, 2), output_padding=(1, 1))
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv_1): ConvTranspose2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
        (norm_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_2): ConvTranspose2d(8, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))
        (norm_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_3): ConvTranspose2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
        (norm_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.01)
      )
    )
    (upconv_1): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
[Mon Aug 07 17:38:53 2023|main.py|INFO] Building optimizer ...
[Mon Aug 07 17:38:53 2023|main.py|INFO] Building criterion ...
[Mon Aug 07 17:38:53 2023|main.py|INFO] Start training ...
[Mon Aug 07 17:38:53 2023|train.py|INFO] Train Epoch: 1
[Mon Aug 07 17:39:05 2023|train.py|INFO] Train Epoch: 1 [0/75568 (0%)]	Loss: 5207818240.000000	KL Loss: 17.620802	Recon Loss: 5207818240.000000
[Mon Aug 07 17:39:25 2023|train.py|INFO] Train Epoch: 1 [15104/75568 (20%)]	Loss: 1565332480.000000	KL Loss: 102.155472	Recon Loss: 1565332352.000000
[Mon Aug 07 17:39:29 2023|train.py|INFO] Train Epoch: 1 [30208/75568 (40%)]	Loss: 1877343744.000000	KL Loss: 151.192734	Recon Loss: 1877343616.000000
[Mon Aug 07 17:39:33 2023|train.py|INFO] Train Epoch: 1 [45312/75568 (60%)]	Loss: 1158258304.000000	KL Loss: 180.816910	Recon Loss: 1158258176.000000
[Mon Aug 07 17:39:37 2023|train.py|INFO] Train Epoch: 1 [60416/75568 (80%)]	Loss: 1142234624.000000	KL Loss: 231.396591	Recon Loss: 1142234368.000000
[Mon Aug 07 17:39:41 2023|train.py|INFO] ====> Epoch: 1 Average loss: 1376489280.3254
[Mon Aug 07 17:39:41 2023|train.py|INFO] Train Epoch: 2
[Mon Aug 07 17:39:49 2023|train.py|INFO] Train Epoch: 2 [0/75568 (0%)]	Loss: 945434368.000000	KL Loss: 251.712769	Recon Loss: 945434112.000000
[Mon Aug 07 17:39:53 2023|train.py|INFO] Train Epoch: 2 [15104/75568 (20%)]	Loss: 1505874176.000000	KL Loss: 238.989807	Recon Loss: 1505873920.000000
[Mon Aug 07 17:39:57 2023|train.py|INFO] Train Epoch: 2 [30208/75568 (40%)]	Loss: 722016192.000000	KL Loss: 297.888855	Recon Loss: 722015872.000000
[Mon Aug 07 17:40:01 2023|train.py|INFO] Train Epoch: 2 [45312/75568 (60%)]	Loss: 600240960.000000	KL Loss: 318.551758	Recon Loss: 600240640.000000
[Mon Aug 07 17:40:05 2023|train.py|INFO] Train Epoch: 2 [60416/75568 (80%)]	Loss: 686781184.000000	KL Loss: 333.498779	Recon Loss: 686780864.000000
[Mon Aug 07 17:40:09 2023|train.py|INFO] ====> Epoch: 2 Average loss: 746960944.2169
[Mon Aug 07 17:40:09 2023|train.py|INFO] Train Epoch: 3
[Mon Aug 07 17:40:17 2023|train.py|INFO] Train Epoch: 3 [0/75568 (0%)]	Loss: 545269952.000000	KL Loss: 348.076477	Recon Loss: 545269632.000000
[Mon Aug 07 17:40:21 2023|train.py|INFO] Train Epoch: 3 [15104/75568 (20%)]	Loss: 514751648.000000	KL Loss: 362.391541	Recon Loss: 514751296.000000
[Mon Aug 07 17:40:25 2023|train.py|INFO] Train Epoch: 3 [30208/75568 (40%)]	Loss: 615788800.000000	KL Loss: 365.267151	Recon Loss: 615788416.000000
[Mon Aug 07 17:40:29 2023|train.py|INFO] Train Epoch: 3 [45312/75568 (60%)]	Loss: 681539392.000000	KL Loss: 381.925293	Recon Loss: 681539008.000000
[Mon Aug 07 17:40:33 2023|train.py|INFO] Train Epoch: 3 [60416/75568 (80%)]	Loss: 602563328.000000	KL Loss: 390.005402	Recon Loss: 602562944.000000
[Mon Aug 07 17:40:37 2023|train.py|INFO] ====> Epoch: 3 Average loss: 684665979.2271
[Mon Aug 07 17:40:37 2023|train.py|INFO] Train Epoch: 4
[Mon Aug 07 17:40:45 2023|train.py|INFO] Train Epoch: 4 [0/75568 (0%)]	Loss: 715749120.000000	KL Loss: 396.370483	Recon Loss: 715748736.000000
[Mon Aug 07 17:40:49 2023|train.py|INFO] Train Epoch: 4 [15104/75568 (20%)]	Loss: 802193280.000000	KL Loss: 403.324860	Recon Loss: 802192896.000000
[Mon Aug 07 17:40:54 2023|train.py|INFO] Train Epoch: 4 [30208/75568 (40%)]	Loss: 639122304.000000	KL Loss: 415.152374	Recon Loss: 639121920.000000
[Mon Aug 07 17:40:58 2023|train.py|INFO] Train Epoch: 4 [45312/75568 (60%)]	Loss: 809337920.000000	KL Loss: 420.266052	Recon Loss: 809337472.000000
[Mon Aug 07 17:41:02 2023|train.py|INFO] Train Epoch: 4 [60416/75568 (80%)]	Loss: 623384512.000000	KL Loss: 425.204590	Recon Loss: 623384064.000000
[Mon Aug 07 17:41:06 2023|train.py|INFO] ====> Epoch: 4 Average loss: 656627542.1288
[Mon Aug 07 17:41:06 2023|train.py|INFO] Train Epoch: 5
[Mon Aug 07 17:41:14 2023|train.py|INFO] Train Epoch: 5 [0/75568 (0%)]	Loss: 632419840.000000	KL Loss: 434.962646	Recon Loss: 632419392.000000
[Mon Aug 07 17:41:18 2023|train.py|INFO] Train Epoch: 5 [15104/75568 (20%)]	Loss: 634060096.000000	KL Loss: 439.840820	Recon Loss: 634059648.000000
[Mon Aug 07 17:41:22 2023|train.py|INFO] Train Epoch: 5 [30208/75568 (40%)]	Loss: 727591168.000000	KL Loss: 443.231140	Recon Loss: 727590720.000000
[Mon Aug 07 17:41:26 2023|train.py|INFO] Train Epoch: 5 [45312/75568 (60%)]	Loss: 645315392.000000	KL Loss: 441.890411	Recon Loss: 645314944.000000
[Mon Aug 07 17:41:30 2023|train.py|INFO] Train Epoch: 5 [60416/75568 (80%)]	Loss: 631602880.000000	KL Loss: 448.976807	Recon Loss: 631602432.000000
[Mon Aug 07 17:41:34 2023|train.py|INFO] ====> Epoch: 5 Average loss: 649528797.9390
[Mon Aug 07 17:41:34 2023|train.py|INFO] Train Epoch: 6
[Mon Aug 07 17:41:42 2023|train.py|INFO] Train Epoch: 6 [0/75568 (0%)]	Loss: 614424704.000000	KL Loss: 452.655640	Recon Loss: 614424256.000000
[Mon Aug 07 17:41:46 2023|train.py|INFO] Train Epoch: 6 [15104/75568 (20%)]	Loss: 954761472.000000	KL Loss: 446.589722	Recon Loss: 954761024.000000
[Mon Aug 07 17:41:51 2023|train.py|INFO] Train Epoch: 6 [30208/75568 (40%)]	Loss: 564207680.000000	KL Loss: 455.606995	Recon Loss: 564207232.000000
[Mon Aug 07 17:41:55 2023|train.py|INFO] Train Epoch: 6 [45312/75568 (60%)]	Loss: 657082944.000000	KL Loss: 460.768799	Recon Loss: 657082496.000000
[Mon Aug 07 17:41:59 2023|train.py|INFO] Train Epoch: 6 [60416/75568 (80%)]	Loss: 661629888.000000	KL Loss: 465.872589	Recon Loss: 661629440.000000
[Mon Aug 07 17:42:03 2023|train.py|INFO] ====> Epoch: 6 Average loss: 659471749.0983
[Mon Aug 07 17:42:03 2023|train.py|INFO] Train Epoch: 7
[Mon Aug 07 17:42:11 2023|train.py|INFO] Train Epoch: 7 [0/75568 (0%)]	Loss: 589022144.000000	KL Loss: 468.983368	Recon Loss: 589021696.000000
[Mon Aug 07 17:42:15 2023|train.py|INFO] Train Epoch: 7 [15104/75568 (20%)]	Loss: 560121792.000000	KL Loss: 470.767242	Recon Loss: 560121344.000000
[Mon Aug 07 17:42:19 2023|train.py|INFO] Train Epoch: 7 [30208/75568 (40%)]	Loss: 617433664.000000	KL Loss: 474.845520	Recon Loss: 617433216.000000
[Mon Aug 07 17:42:23 2023|train.py|INFO] Train Epoch: 7 [45312/75568 (60%)]	Loss: 516896576.000000	KL Loss: 475.826904	Recon Loss: 516896096.000000
[Mon Aug 07 17:42:27 2023|train.py|INFO] Train Epoch: 7 [60416/75568 (80%)]	Loss: 718421632.000000	KL Loss: 486.619141	Recon Loss: 718421120.000000
[Mon Aug 07 17:42:31 2023|train.py|INFO] ====> Epoch: 7 Average loss: 646353448.7322
[Mon Aug 07 17:42:31 2023|train.py|INFO] Train Epoch: 8
[Mon Aug 07 17:42:39 2023|train.py|INFO] Train Epoch: 8 [0/75568 (0%)]	Loss: 585749120.000000	KL Loss: 480.454102	Recon Loss: 585748608.000000
[Mon Aug 07 17:42:43 2023|train.py|INFO] Train Epoch: 8 [15104/75568 (20%)]	Loss: 686485760.000000	KL Loss: 487.948364	Recon Loss: 686485248.000000
[Mon Aug 07 17:42:47 2023|train.py|INFO] Train Epoch: 8 [30208/75568 (40%)]	Loss: 693828352.000000	KL Loss: 489.492493	Recon Loss: 693827840.000000
[Mon Aug 07 17:42:51 2023|train.py|INFO] Train Epoch: 8 [45312/75568 (60%)]	Loss: 579119616.000000	KL Loss: 493.790405	Recon Loss: 579119104.000000
[Mon Aug 07 17:42:55 2023|train.py|INFO] Train Epoch: 8 [60416/75568 (80%)]	Loss: 505829984.000000	KL Loss: 495.912933	Recon Loss: 505829504.000000
[Mon Aug 07 17:42:59 2023|train.py|INFO] ====> Epoch: 8 Average loss: 640862574.9695
[Mon Aug 07 17:42:59 2023|train.py|INFO] Train Epoch: 9
[Mon Aug 07 17:43:07 2023|train.py|INFO] Train Epoch: 9 [0/75568 (0%)]	Loss: 590064512.000000	KL Loss: 505.075806	Recon Loss: 590064000.000000
[Mon Aug 07 17:43:11 2023|train.py|INFO] Train Epoch: 9 [15104/75568 (20%)]	Loss: 627717376.000000	KL Loss: 503.090515	Recon Loss: 627716864.000000
[Mon Aug 07 17:43:15 2023|train.py|INFO] Train Epoch: 9 [30208/75568 (40%)]	Loss: 601686848.000000	KL Loss: 515.513916	Recon Loss: 601686336.000000
[Mon Aug 07 17:43:19 2023|train.py|INFO] Train Epoch: 9 [45312/75568 (60%)]	Loss: 598538112.000000	KL Loss: 515.392517	Recon Loss: 598537600.000000
[Mon Aug 07 17:43:23 2023|train.py|INFO] Train Epoch: 9 [60416/75568 (80%)]	Loss: 559604608.000000	KL Loss: 518.551086	Recon Loss: 559604096.000000
[Mon Aug 07 17:43:28 2023|train.py|INFO] ====> Epoch: 9 Average loss: 630274394.6847
[Mon Aug 07 17:43:28 2023|train.py|INFO] Train Epoch: 10
[Mon Aug 07 17:43:36 2023|train.py|INFO] Train Epoch: 10 [0/75568 (0%)]	Loss: 572119424.000000	KL Loss: 518.627319	Recon Loss: 572118912.000000
[Mon Aug 07 17:43:39 2023|train.py|INFO] Train Epoch: 10 [15104/75568 (20%)]	Loss: 706562688.000000	KL Loss: 529.449524	Recon Loss: 706562176.000000
[Mon Aug 07 17:43:43 2023|train.py|INFO] Train Epoch: 10 [30208/75568 (40%)]	Loss: 677148544.000000	KL Loss: 530.151611	Recon Loss: 677148032.000000
[Mon Aug 07 17:43:47 2023|train.py|INFO] Train Epoch: 10 [45312/75568 (60%)]	Loss: 615845632.000000	KL Loss: 533.363586	Recon Loss: 615845120.000000
[Mon Aug 07 17:43:51 2023|train.py|INFO] Train Epoch: 10 [60416/75568 (80%)]	Loss: 691160704.000000	KL Loss: 542.304932	Recon Loss: 691160192.000000
[Mon Aug 07 17:43:55 2023|train.py|INFO] ====> Epoch: 10 Average loss: 633517174.7797
[Mon Aug 07 17:43:55 2023|train.py|INFO] Model saved at checkpoints\VAE-latentdim_128-sigma_0.001-bg_0.5\2023-08-07-17-38-53\VAE-Epoch_10-Loss_633517174.7797.pth
